{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import copy\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Описательная часть методологии формирования обучающего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основные аспекты методики формирования датасета:\n",
    "- учет влияния каждого компонента (т.е. тикера)\n",
    "- аккумулирование в часовые таймфреймы параметров из минутных данных\n",
    "- отказ от применения стандартных временных рядов как единственных входных данных в модель МО.\n",
    "\n",
    "\n",
    "Что является ***общепринятым подходом*** технического анализа цены акции? ***Применение технических индикаторов***, налагаемых на временной ряд цены.\n",
    "\n",
    "Излишне пояснять минус такого подхода. ***Основным минусом является зависимость показаний индикаторов исключительно от предыдущих колебаний цены единственной акции***. Эти колебания, в свою очередь, обусловлены множеством других факторов. Не последнюю, а может и самую важную роль, играет текущий индекс рынка. То есть инвесторы учитывают фазу рынка и корреляцию с другими инструментами рынка. То есть первая акция коррелирует со второй, вторая - с третьей, и так далее. Истинные причины (какая акция на какую влияет, и почему, и влияет ли вообще) остаются в большинстве случаев от нас скрытыми. То есть, опираясь на индикатор по одной акции, мы опираемся как бы на хвост, последний вагон поезда. Куда же поезд держит путь, какого размера состав поезда, какова политика локомотива - от нас остается скрытым. Цена акции не дает нам такого ответа.\n",
    "\n",
    "В сложившейся ситуации многофакторного взаимного влияния можно избрать ***следующий подход, легко моделируемый математически и вполне понятный на обывательском уровне.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы беремся управлять пониманием всего рынка в целом (точнее, 500 его основных компонентов). Чтобы ни одно движение ни одного из 500 компонентов не ускользнуло от нашего внимания, и мы смогли принимать более взвешенные и статистически подкрепленные решения. Если нам это удастся, ***мы сможем вместо инвестирования в \"хвосты поезда\", т.е. отдельные акции, инвестировать в ETF SP&500.***\n",
    "\n",
    "На обывательском уровне идея проста и понятна. Давайте попробуем составить математический каркас одного из многочисленных способов реализации этой идеи.\n",
    "\n",
    "(Не будем забывать следующее. Идей, которые можно выдвинуть на финансовых рынках - сотни, тысячи. Конкретных способов реализации этой идей - тоже тысячи. Итого миллионы возможных путей по которому мы могли пойти. С учетом этого, вопрос **\"а почему выбрано именно такое действие? можно было по-другому\" получает автоматический ответ \"вообще-то есть еще миллионы вариантов действий. сейчас мы рассматриваем один вариант\".**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, какие же конкретно шаги мы предпримем. Берем отдельный тикер, и поминутно в течение часа собираем по нему **количество признаков `highes`, свидетельствующих о желательности входа в позицию** Лонг (мы ждем роста рынка в ближайшие минуты). Отдельно собираем количество признаков **`lowes`** для позиции Шорт (мы ждем падения рынка в ближайшие минуты). И так по каждому тикеру. В итоге суммируем количество признаков отдельно для позиций Лонг и Шорт.\n",
    "\n",
    "Таким образом, мы получаем для характеристики текущего часа два числа. Существенное превышение числа **`highes`** над числом **`lowes`** даст нам повод предположить, что рынок в ближайшие часы будет расти. И наоборот, существенное превышение **`lowes`** над **`highes`** даст повод думать, что рынок будет падать.\n",
    "\n",
    "Ну и чтобы совсем не отказываться от такого фактора как цена, вводим третью характеристику **`market_index_rolling`** - изменение уровня/цены индекса S&P 500 текущего часа по сравнению со средним уровнем за последние 32 часа, допустим. Это позволит модели МО соотносить числа **`lowes`** и **`highes`** в каждом часе с тем, насколько все это влияет на уровень индекса в следующем часе. И то, что мы берем изменение индекса, а не сам индекс, позволяет нам добиться стационарности данных, что необходимо для модели МО.\n",
    "\n",
    "Но мы же помним, что работаем с временными данными. При формировании датасета для модели МО, и тем более при применении нейронных LSTM-сетей или GRU-сетей нам нужно подавать на вход временной ряд. Поэтому нам из наших данных за текущий час (**`lowes`**, **`highes`**, **`market_index_rolling`**) нужно получить аналог временных данных. Как? Мы просто берем эти данные не за 1 час, а за те же последние 32 часа, допустим. В итоге получаем 3 * 32 = 96 итоговых признаков для формирования датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что же будет целевым столбцом - т.н. таргетом? В нашем случае таргетом будет и само изменение цены (то есть регрессионная задача МО), и отдельно продемонстрируем пример рекомендательной системы как решение задачи классификации с 3-мя классами (то есть покупать/продавать, или же удержание позиции \"холд\" - ничего не делать)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Важное примечание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно помнить, что такой параметр как - \"поминутное количество признаков, свидетельствующих о желательности входа в позицию\" - это абсолютно опциональная вещь. Это параметр может реализовать собственноручно написанная единственная функция в три строки, которая в корне поменяет результативность исследования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижеприведенный код реализует описанную выше логику. В этой части мы формируем датасет с примером формирования таргета в виде классов. Обучение модели МО на основе датасета и анализ, интерпретация результатов показаны в следующем файле ноутбука."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3. Функциональная часть кода формирования датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные: загрузка\n",
    "def traindata_making_standard():\n",
    "    path = './df_marketdata/'\n",
    "    companies_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for _file in files:\n",
    "            companies_list.append(_file)\n",
    "    companies_list = sorted(companies_list)\n",
    "    DF = {}\n",
    "    for company in companies_list:\n",
    "        try:\n",
    "            df = joblib.load('./df_marketdata/'+company)\n",
    "            DF[company] = df\n",
    "        except:\n",
    "            print(company)\n",
    "            pass    \n",
    "    \n",
    "    return DF\n",
    "\n",
    "#Данные: ресэмплирование на нужные минутные интервалы перед подсчетом точек\n",
    "def resampling_standard_data(DF, minutes=5):\n",
    "    \n",
    "    resample_time=str(minutes)+str('Min')\n",
    "\n",
    "    DF_1={}\n",
    "    for i in DF.keys():\n",
    "        try:\n",
    "\n",
    "            df1=DF[i].copy(deep=True)\n",
    "            df=pd.DataFrame()\n",
    "            df['High'], df['Low']=df1['High'].resample(resample_time).max(), df1['Low'].resample(resample_time).min()\n",
    "            df['company']=str(i)\n",
    "            df=df[df['High'].notnull()]\n",
    "\n",
    "\n",
    "            DF_1[i]=df\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    return DF_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ПОДСЧЕТ МОМЕНТОВ УВЕРЕННОГО РАЗВОРОТА ПО РЕСЭМПЛИРОВАННЫМ ТОЧКАМ - по одному тикеру\n",
    "# Возвращается список таймстэмпов\n",
    "def get_high_low_timestamps_for_one_df(df_newest):\n",
    "    spisok_low=[]\n",
    "    spisok_high=[]\n",
    "    for y in range(10, len(df_newest)):\n",
    "        try:\n",
    "            if df_newest['Srlow'].iat[y] >= 0:\n",
    "                y_point=y\n",
    "                y=y-1\n",
    "                while not df_newest['Srlow'].iat[y] >= 0:\n",
    "                    y=y-1\n",
    "                srlow3=y\n",
    "\n",
    "                y=y-1\n",
    "                while not df_newest['Srlow'].iat[y] >= 0:\n",
    "                    y=y-1\n",
    "                srlow2=y\n",
    "\n",
    "                y=y_point-1                     \n",
    "                while not df_newest['Srhigh'].iat[y] >= 0:\n",
    "                    y=y-1\n",
    "                srhigh=y                    \n",
    "\n",
    "                if srhigh<srlow3:\n",
    "                    if df_newest['Srlow'].iat[y_point]>df_newest['Srlow'].iat[srlow3] and df_newest['Srlow'].iat[srlow2]>df_newest['Srlow'].iat[srlow3]:\n",
    "                        moment_low=df_newest.index[srhigh]\n",
    "                        spisok_low.append(moment_low)\n",
    "        except:\n",
    "            pass\n",
    "    # а теперь по моментам хай    \n",
    "    for y in range(10, len(df_newest)):\n",
    "        try:\n",
    "            if df_newest['Srhigh'].iat[y] >= 0:\n",
    "                y_point=y\n",
    "                y=y-1\n",
    "                while not df_newest['Srhigh'].iat[y] >= 0:\n",
    "                    y=y-1\n",
    "                srhigh3=y\n",
    "\n",
    "                y=y-1\n",
    "                while not df_newest['Srhigh'].iat[y] >= 0:\n",
    "                    y=y-1\n",
    "                srhigh2=y\n",
    "\n",
    "                y=y_point-1                     \n",
    "                while not df_newest['Srlow'].iat[y] >= 0:\n",
    "                    y=y-1\n",
    "                srlow=y                    \n",
    "\n",
    "                if srlow<srhigh3:\n",
    "                    if df_newest['Srhigh'].iat[y_point]<df_newest['Srhigh'].iat[srhigh3] and df_newest['Srhigh'].iat[srhigh2]<df_newest['Srhigh'].iat[srhigh3]:\n",
    "                        moment_high=df_newest.index[srlow]\n",
    "                        spisok_high.append(moment_high)\n",
    "        except:\n",
    "            pass\n",
    "    return spisok_high, spisok_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получение полного списка таймстэмпов по всем тикерам\n",
    "def get_timestamps_for_all(traindata_resampled):\n",
    "    companies = traindata_resampled.keys()\n",
    "    spisok_high_global = []\n",
    "    spisok_low_global = []\n",
    "    for company in companies:\n",
    "        df=traindata_resampled[company].copy(deep=True)\n",
    "        df_newest = get_fractal_points_for_one_df(df)\n",
    "        spisok_high, spisok_low = get_high_low_timestamps_for_one_df(df_newest)\n",
    "        spisok_high_global.append(spisok_high)\n",
    "        spisok_low_global.append(spisok_low)\n",
    "    spisok_high_all = []\n",
    "    spisok_low_all = []    \n",
    "    for item in spisok_high_global:\n",
    "        for i in item:\n",
    "            spisok_high_all.append(i)\n",
    "    for item in spisok_low_global:\n",
    "        for i in item:\n",
    "            spisok_low_all.append(i)            \n",
    "            \n",
    "    \n",
    "    return spisok_high_all, spisok_low_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получение чернового датасета по минутам\n",
    "def get_unsampled_dataset(traindata_standard, times = [3,7,15]):\n",
    "    step = 0\n",
    "    for minute in times:\n",
    "        traindata_resampled = resampling_standard_data(traindata_standard, minutes=minute)\n",
    "        spisok_high_all, spisok_low_all = get_timestamps_for_all(traindata_resampled)\n",
    "        if step == 0:\n",
    "            max_time = max(max(spisok_high_all), max(spisok_low_all))\n",
    "            min_time = min(min(spisok_high_all), min(spisok_low_all))\n",
    "            indexes=pd.date_range(start=min_time, end=max_time, freq='1Min')\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            dfs = pd.DataFrame(index=indexes)\n",
    "            dfs['day_number'] = dfs.index.date\n",
    "            dti=pd.date_range(start=min_time, end=max_time, freq='B')\n",
    "            dfs_filtered=(dfs.loc[dfs['day_number'].isin(pd.DataFrame(index=dti).index.date)])\n",
    "            dfs_filtered_between_time=dfs_filtered.between_time(start_time='09:30', end_time='16:00')\n",
    "            \n",
    "            \n",
    "            unsampled_dataset = pd.DataFrame(index=dfs_filtered_between_time.index)\n",
    "        step = step + 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        slovar_high = Counter(spisok_high_all)\n",
    "        slovar_low = Counter(spisok_low_all)\n",
    "\n",
    "        unsampled_dataset['highes'+str(minute)] = pd.Series(data=list(slovar_high.values()), index=list(slovar_high.keys()))\n",
    "        unsampled_dataset['lowes'+str(minute)] = pd.Series(data=list(slovar_low.values()), index=list(slovar_low.keys()))\n",
    "\n",
    "    return unsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Данные: после ресэмплирования, получение точек для одного тикера\n",
    "def get_fractal_points_for_one_df(df):\n",
    "    \n",
    "    hh=(df['High']>df['Low'].rolling(3).mean())&(df['High']>df['Low'].shift(-1)*1.002)\n",
    "    ll=(df['High']<df['Low'].rolling(3).mean()*1.01)&(df['High']<df['Low'].shift(-1)*1.05)\n",
    "\n",
    "    df_new_hh=pd.DataFrame({'High':df[hh].High})\n",
    "    frac_hh=(df_new_hh['High']>df_new_hh['High'].shift(-4))&(df_new_hh['High']>df_new_hh['High'].shift(-4))\n",
    "\n",
    "    df_new_ll=pd.DataFrame({'Low':df[ll].Low})\n",
    "    frac_ll=(df_new_ll['Low']<df_new_ll['Low'].rolling(10).mean()*1.01)&(df_new_ll['Low']<df_new_ll['Low'].shift(-1))\n",
    "\n",
    "    df_newest=pd.DataFrame({'Srhigh':df_new_hh[frac_hh].High,'Srlow':df_new_ll[frac_ll].Low})\n",
    "    \n",
    "    return df_newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательная функция. применяется в случае тренировки данных из разных источников\n",
    "def traindata_standard_together(traindata_standard_previous, traindata_standard_current):\n",
    "    traindata_standard={}\n",
    "    for company in traindata_standard_previous.keys():\n",
    "        try:\n",
    "            df = traindata_standard_previous[company]\n",
    "            traindata_standard[company]=df.append(traindata_standard_current[company])\n",
    "        except:\n",
    "            pass\n",
    "    return traindata_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ресэмплирование к периоду 1 час всех периодов (3, 7, 15 минут) - методом суммы по каждому периоду\n",
    "#оставляем только время от 09.00 до 16.00\n",
    "def get_resampled_dataset(unsampled_dataset):\n",
    "    resampled_dataset = unsampled_dataset.resample('H', label = 'right').sum()\n",
    "    return resampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#формирование датасета из цен акций - для получения индекса рынка по часам\n",
    "def get_dataset_for_market_index(traindata_standard):\n",
    "    step = 0\n",
    "    companies = traindata_standard.keys()\n",
    "    DF={}\n",
    "    for company in companies:\n",
    "        df=traindata_standard[company].copy(deep=True)\n",
    "        df['price'] = (df['High'] + df['Low']) / 2\n",
    "        #df.drop(columns=['High', 'Low', 'company'], inplace=True)\n",
    "        DF[company] = df['price']\n",
    "    pddf=pd.DataFrame(DF)\n",
    "    df=pddf.resample('H', label = 'right').mean()\n",
    "\n",
    "    if step == 0:\n",
    "        indexes=pd.date_range(start=df.index[0], end=df.index[-1], freq='H')\n",
    "        dfs = pd.DataFrame(index=indexes)\n",
    "        dfs['day_number'] = dfs.index.date\n",
    "        dti=pd.date_range(start=df.index[0], end=df.index[-1], freq='B')\n",
    "        dfs_filtered=(dfs.loc[dfs['day_number'].isin(pd.DataFrame(index=dti).index.date)])\n",
    "        dfs_filtered_between_time=dfs_filtered.between_time(start_time='09:30', end_time='16:00')\n",
    "    df_filtered = (df.index.isin(dfs_filtered_between_time.index))\n",
    "    pddf=df[df_filtered]\n",
    "    \n",
    "    \n",
    "    step = step + 1    \n",
    "    \n",
    "    dataset_for_market_index=pddf/pddf.shift(1)-1\n",
    "    dataset_for_market_index=(1+dataset_for_market_index).cumprod()\n",
    "    return dataset_for_market_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получение индекса рынка по часам\n",
    "def get_market_index_by_hour(dataset_for_market_index):\n",
    "    return dataset_for_market_index.transpose().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получение изменения среднего индекса рынка по часам (32 часа)\n",
    "def get_market_index_rolling(market_index_by_hour):\n",
    "    market_index_by_hour.dropna(inplace=True)\n",
    "    market_index_rolling = market_index_by_hour - market_index_by_hour.rolling(32).mean()\n",
    "    return market_index_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# справочная вспомогательная функция\n",
    "def get_market_index_for_plotting(market_index_by_hour):\n",
    "    market_index_by_hour.dropna(inplace=True)\n",
    "    market_index_for_plotting = market_index_by_hour\n",
    "    return market_index_for_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получение датасета (с индексом рынка и только по рабочим бизнес-дням) для вычисления финального датасета\n",
    "def get_dataset_for_calculation(resampled_dataset, market_index_rolling):\n",
    "    resampled_dataset['market_index_rolling'] = market_index_rolling\n",
    "    mask = (resampled_dataset['market_index_rolling'].notnull())\n",
    "    dataset_for_calculation = resampled_dataset[mask]\n",
    "    return dataset_for_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получаем имена столбцов датасета\n",
    "def get_224__columnnames_for_dataset():\n",
    "    columns_mask = ['highes3','lowes3','highes7','lowes7','highes15','lowes15','market_index_rolling']\n",
    "    global_mask = []\n",
    "    for i in range(32):\n",
    "        mask = []\n",
    "        for j in columns_mask:\n",
    "            imya = str(i) + '_' + str(j)\n",
    "            mask.append(imya)\n",
    "        global_mask = global_mask + mask\n",
    "    return global_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#простираем вправо на 32 часа прежний датасет, с именами столбцов. датасет уже для маркировки\n",
    "def get_ready_dataset_for_marking(dataset_for_calculation):\n",
    "    calculated_dataset_for_marking = []\n",
    "    dataset_for_marking_indexes = []\n",
    "    for row in range(31, len(dataset_for_calculation)):\n",
    "        row_list=[]\n",
    "        for item in range(32):\n",
    "            row_list = row_list + list(dataset_for_calculation.iloc[row - item].values)\n",
    "        calculated_dataset_for_marking.append(row_list)\n",
    "        dataset_for_marking_indexes.append(dataset_for_calculation.index[row])\n",
    "    global_mask = get_224__columnnames_for_dataset()\n",
    "    ready_dataset_for_marking = pd.DataFrame(data=calculated_dataset_for_marking, index=dataset_for_marking_indexes, columns=global_mask)\n",
    "    return ready_dataset_for_marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем первоначальный таргет-столбец с 4-мя классами. служит для иллюстрации\n",
    "# способов составления таргет-столбца. в дальнейшем будут более\n",
    "# релевантные поведению инвестора функции для таргет-столбцов.\n",
    "# столбец отражает, что случится раньше:\n",
    "    # - падение рынка более чем на 1% и дальнейший рост более чем на 4%\n",
    "    # - падение рынка более чем на 1% и дальнейшее падение более чем на 2%\n",
    "    # - рост рынка более чем на 1% и дальнейший рост более чем на 4%\n",
    "    # - рост рынка более чем на 1% и дальнейшее падение более чем на 2%\n",
    "def get_df_rates_1percent(market_index_by_hour):\n",
    "    df = pd.DataFrame(market_index_by_hour)\n",
    "    df['rates']=''\n",
    "    df_list = list(market_index_by_hour)\n",
    "    for i in range(len(df_list)):\n",
    "        for k in range(i+1, len(df_list)):\n",
    "            delta = df_list[k] - df_list[i]\n",
    "            if delta/df_list[i] >0.01:\n",
    "                df['rates'].iloc[i]=1\n",
    "                break\n",
    "            if delta/df_list[i] <-0.01:\n",
    "                df['rates'].iloc[i]=-1\n",
    "                break\n",
    "    df = df[df['rates']!='']\n",
    "    \n",
    "    df['rates_add']=''\n",
    "    df_list = list(market_index_by_hour)\n",
    "    for i in range(len(df_list)):\n",
    "        for k in range(i+1, len(df_list)):\n",
    "            delta = df_list[k] - df_list[i]\n",
    "            if delta/df_list[i] >0.04:\n",
    "                df['rates_add'].iloc[i]=2\n",
    "                break\n",
    "            if delta/df_list[i] <-0.02:\n",
    "                df['rates_add'].iloc[i]=-2\n",
    "                break\n",
    "    df = df[df['rates_add']!='']\n",
    "    \n",
    "    df['total_rates'] = df['rates'] + df['rates_add']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df['total_rates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получение столбца для добавления - индекс рынка по часам\n",
    "def get_market_index_by_hour_updated():\n",
    "    df = joblib.load('SP.pkl')\n",
    "    return df.Price\n",
    "def get_market_index_rolling(market_index_by_hour):\n",
    "    market_index_by_hour.dropna(inplace=True)\n",
    "    market_index_rolling = market_index_by_hour - market_index_by_hour.rolling(32).mean()\n",
    "    return market_index_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формирование датасета с именами столбцов, и с таргетом\n",
    "def percent_get_ready_dataset(dataset_for_calculation):\n",
    "    calculated_dataset_for_marking = []\n",
    "    dataset_for_marking_indexes = []\n",
    "    for row in range(31, len(dataset_for_calculation)):\n",
    "        row_list=[]\n",
    "        for item in range(32):\n",
    "            row_list = row_list + list(dataset_for_calculation.iloc[row - item].values)\n",
    "        calculated_dataset_for_marking.append(row_list)\n",
    "        dataset_for_marking_indexes.append(dataset_for_calculation.index[row])\n",
    "    global_mask = get_224__columnnames_for_dataset()\n",
    "    temp = pd.DataFrame(data=calculated_dataset_for_marking, index=dataset_for_marking_indexes, columns=global_mask)\n",
    "    temp['class'] = get_df_rates_1percent(market_index_by_hour)\n",
    "    percent_ready_dataset = temp[temp['class'].notnull()]\n",
    "    return percent_ready_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 4. Исполняемая часть кода. Запуск функций для формирования датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_standard = traindata_making_standard()\n",
    "unsampled_dataset = get_unsampled_dataset(traindata_standard, times = [3,7,15])\n",
    "resampled_dataset = get_resampled_dataset(unsampled_dataset)\n",
    "dataset_for_market_index = get_dataset_for_market_index(traindata_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_index_by_hour=get_market_index_by_hour_updated()\n",
    "market_index_rolling = get_market_index_rolling(market_index_by_hour)\n",
    "dataset_for_calculation = get_dataset_for_calculation(resampled_dataset, market_index_rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = percent_get_ready_dataset(dataset_for_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 5. Получаем таргетированный столбец в виде регрессионной задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многочисленные опыты с обучением полученного датасета не привели к адекватной результативности на тестовых данных. Причина видится в логике таргет-столбца. Таргет не учитывает, что цена в течение ближайших часов может вырасти как на 1%, так и на 3%. Разница колоссальная, а маркировка (то есть метка класса) одинаковая. Большее количество классов для учета всех ситуаций ведет к усложнению обучения модели МО. Поэтому рискнем и предположим таргет-столбец в виде самой цены. Точнее, разницы между `High` последующего часа и средней ценой `(High+Low)/2` текущего часа. Ниже пример вычисления:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "High         | Low         | Target_class\n",
    ":------------|:------------|:------------\n",
    "     15      |     11      |  5 = 18-(15+11)/2\n",
    "     18      |     12      |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время загрузить вспомогательный файл `'SP.pkl'`. Это SP&500 по часам.\n",
    "Столбец `SP['Price']` в нем как раз и является вычисляемым как `(High+Low)/2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Price_high</th>\n",
       "      <th>Price_low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2008-01-02 10:00:00</td>\n",
       "      <td>1469.825</td>\n",
       "      <td>1471.77</td>\n",
       "      <td>1467.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-01-02 11:00:00</td>\n",
       "      <td>1461.170</td>\n",
       "      <td>1465.26</td>\n",
       "      <td>1457.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-01-02 12:00:00</td>\n",
       "      <td>1453.490</td>\n",
       "      <td>1458.26</td>\n",
       "      <td>1448.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-01-02 13:00:00</td>\n",
       "      <td>1449.015</td>\n",
       "      <td>1452.37</td>\n",
       "      <td>1445.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-01-02 14:00:00</td>\n",
       "      <td>1447.170</td>\n",
       "      <td>1450.37</td>\n",
       "      <td>1443.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-07-26 12:00:00</td>\n",
       "      <td>3020.180</td>\n",
       "      <td>3023.46</td>\n",
       "      <td>3016.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-07-26 13:00:00</td>\n",
       "      <td>3022.855</td>\n",
       "      <td>3023.93</td>\n",
       "      <td>3021.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-07-26 14:00:00</td>\n",
       "      <td>3023.375</td>\n",
       "      <td>3024.52</td>\n",
       "      <td>3022.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-07-26 15:00:00</td>\n",
       "      <td>3024.565</td>\n",
       "      <td>3026.64</td>\n",
       "      <td>3022.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-07-26 16:00:00</td>\n",
       "      <td>3026.835</td>\n",
       "      <td>3027.98</td>\n",
       "      <td>3025.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20302 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Price  Price_high  Price_low\n",
       "Date_time                                           \n",
       "2008-01-02 10:00:00  1469.825     1471.77    1467.88\n",
       "2008-01-02 11:00:00  1461.170     1465.26    1457.08\n",
       "2008-01-02 12:00:00  1453.490     1458.26    1448.72\n",
       "2008-01-02 13:00:00  1449.015     1452.37    1445.66\n",
       "2008-01-02 14:00:00  1447.170     1450.37    1443.97\n",
       "...                       ...         ...        ...\n",
       "2019-07-26 12:00:00  3020.180     3023.46    3016.90\n",
       "2019-07-26 13:00:00  3022.855     3023.93    3021.78\n",
       "2019-07-26 14:00:00  3023.375     3024.52    3022.23\n",
       "2019-07-26 15:00:00  3024.565     3026.64    3022.49\n",
       "2019-07-26 16:00:00  3026.835     3027.98    3025.69\n",
       "\n",
       "[20302 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP = joblib.load('SP.pkl')\n",
    "SP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, приходим к задаче регрессии.\n",
    "Нижеприведенная функция получения таргет-столбца реализует эту логику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_with_high_average_delta(dataset):\n",
    "    SP = joblib.load('SP.pkl') # данный файл подготовлен на основе данных\n",
    "                               # finam.ru\n",
    "    temp = pd.DataFrame(index=dataset.index)\n",
    "    temp['Price_high'] = SP['Price_high']\n",
    "    temp['Price'] = SP['Price'] # это и есть цена (High+Low)/2\n",
    "    temp['class'] = temp.Price_high.shift(-1) - temp.Price\n",
    "    return temp['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['class'] = get_regression_with_high_average_delta(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:-1] #избавляемся от последней строки с Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут конечно надо дать пояснения. Мы сделали очень серьезное допущение, приемлемое для целей машинного обучения. Но на этапе продакшна это может вызвать серьезную озабоченность.\n",
    "\n",
    "Итак, речь идет о формуле: **таргет-столбец равен разнице между `High` последующего часа и средней ценой `(High+Low)/2` текущего часа**.\n",
    "\n",
    "На практике имело бы смысл для текущего часа брать цену **`Close`**, так как именно от нее будет считаться финансовый результат. Именно по этой цене мы имеем все шансы войти в сделку. Насколько **`(High+Low)/2`** помешает нашим целям?\n",
    "\n",
    "Предварительный ответ: не помешает. По статистическим причинам:\n",
    "- В среднем, на протяжении длительного периода времени, цена **`Close`** практически совпадает с условной ценой **`(High+Low)/2`**. Матожидание отклонений составляет минус 0,1 пункт. Стандартное отклонение - 3 пункта. И это при уровне SP&500 2000-3000 пунктов. То есть, при рекомендациях модели, в какие-то моменты мы будем недобирать пару пунктов, в другие моменты - забирать себе эти лишние пункты. В среднем будем выходить на планируемые уровни прибыли.\n",
    "- На бэктесте этот момент надо проверять, в любом случае. Вдруг модель будет выбирать в основном такие моменты, где мы будем терять на этих отклонениях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но зачем мы применили именно **`(High+Low)/2`** вместо привычного **`Close`**? Все по тем же причинам - общая концепция моделирования датасета. Датасет весь построен на отклонениях **`High и Low`**. Дело в том, что эти два уровня цен характеризуют весь временной период (минута, час, день). Показывают, насколько высоко ушел или низко упал рынок. То есть содержат сжатую информацию обо всем периоде. Между тем **`Close`** характеризует только один момент времени. **`Close`** статичен по своей природе, в отличие от динамичных **`High и Low`**.\n",
    "\n",
    "При наличии цены **`Close`** мы получим только одну информацию на конец часа. Как закрылись торги в этом часе. А что происходило внутри часа, какие баталии, насколько рынок уходил вверх или падал вниз - мы не узнаем. Точнее даже не мы, а модель МО, для которой подобная информация (**`High и Low`**) может послужить ценным источником.\n",
    "\n",
    "Таким образом, теряя (возможно) пару пунктов из-за принятого допущения, мы приобретаем улучшенную точность модели. И любые расчеты показывают, что **повышение точности модели перекроет любые условные допущения и связанные с ними потери**. Это прагматичный расчет, ничего личного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 6. Сокращение размерности датасета, и сохранение окончательного датасета для машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вышеприведенный код реализовал подсчет минут на основе 3-х, 7-и, 15-и минутных интервалов. Практика показала, что без существенных потерь можно снизить размерность датасета до 96 признаков, опираясь только на 3-х минутные интервалы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_columns(dataset):\n",
    "    for i in dataset.columns[:-1]:\n",
    "        if ('s7' in i) or ('s15' in i):\n",
    "            dataset.drop(columns=[i], inplace=True)\n",
    "        else:\n",
    "            pass\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = delete_columns(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(dataset, 'marked_dataset_for_regression_with_high_average_delta_.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
